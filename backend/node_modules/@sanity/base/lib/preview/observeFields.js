"use strict";

Object.defineProperty(exports, "__esModule", {
  value: true
});
exports.default = cachedObserveFields;

var _memoize2 = _interopRequireDefault(require("lodash/memoize"));

var _flatten2 = _interopRequireDefault(require("lodash/flatten"));

var _difference2 = _interopRequireDefault(require("lodash/difference"));

var _rxjs = require("rxjs");

var _operators = require("rxjs/operators");

var _versionedClient = require("../client/versionedClient");

var _crossProjectToken = require("../datastores/crossProjectToken");

var _debounceCollect = require("./utils/debounceCollect");

var _optimizeQuery = require("./utils/optimizeQuery");

var _constants = require("./constants");

var _hasEqualFields = _interopRequireDefault(require("./utils/hasEqualFields"));

var _isUniqueBy = _interopRequireDefault(require("./utils/isUniqueBy"));

var _ = require("./");

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

function _slicedToArray(arr, i) { return _arrayWithHoles(arr) || _iterableToArrayLimit(arr, i) || _unsupportedIterableToArray(arr, i) || _nonIterableRest(); }

function _nonIterableRest() { throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method."); }

function _unsupportedIterableToArray(o, minLen) { if (!o) return; if (typeof o === "string") return _arrayLikeToArray(o, minLen); var n = Object.prototype.toString.call(o).slice(8, -1); if (n === "Object" && o.constructor) n = o.constructor.name; if (n === "Map" || n === "Set") return Array.from(o); if (n === "Arguments" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return _arrayLikeToArray(o, minLen); }

function _arrayLikeToArray(arr, len) { if (len == null || len > arr.length) len = arr.length; for (var i = 0, arr2 = new Array(len); i < len; i++) { arr2[i] = arr[i]; } return arr2; }

function _iterableToArrayLimit(arr, i) { var _i = arr == null ? null : typeof Symbol !== "undefined" && arr[Symbol.iterator] || arr["@@iterator"]; if (_i == null) return; var _arr = []; var _n = true; var _d = false; var _s, _e; try { for (_i = _i.call(arr); !(_n = (_s = _i.next()).done); _n = true) { _arr.push(_s.value); if (i && _arr.length === i) break; } } catch (err) { _d = true; _e = err; } finally { try { if (!_n && _i["return"] != null) _i["return"](); } finally { if (_d) throw _e; } } return _arr; }

function _arrayWithHoles(arr) { if (Array.isArray(arr)) return arr; }

var _globalListener;

var getGlobalEvents = () => {
  if (!_globalListener) {
    var allEvents$ = (0, _rxjs.from)(_versionedClient.versionedClient.listen('*[!(_id in path("_.**"))]', {}, {
      events: ['welcome', 'mutation'],
      includeResult: false,
      visibility: 'query',
      tag: 'preview.global'
    })).pipe((0, _operators.share)()); // This is a stream of welcome events from the server, each telling us that we have established listener connection
    // We map these to snapshot fetch/sync. It is good to wait for the first welcome event before fetching any snapshots as, we may miss
    // events that happens in the time period after initial fetch and before the listener is established.

    var welcome$ = allEvents$.pipe((0, _operators.filter)(event => event.type === 'welcome'), (0, _operators.publishReplay)(1), (0, _operators.refCount)()); // This will keep the listener active forever and in turn reduce the number of initial fetches
    // as less 'welcome' events will be emitted.
    // @todo: see if we can delay unsubscribing or connect with some globally defined shared listener

    welcome$.subscribe();
    var mutations$ = allEvents$.pipe((0, _operators.filter)(event => event.type === 'mutation'));
    _globalListener = {
      welcome$,
      mutations$
    };
  }

  return _globalListener;
};

function listen(id) {
  var globalEvents = getGlobalEvents();
  return (0, _rxjs.merge)(globalEvents.welcome$, globalEvents.mutations$.pipe((0, _operators.filter)(event => event.documentId === id)));
}

function fetchAllDocumentPathsWith(client, token) {
  var headers = token ? {
    'sanity-project-tokens': "".concat(token.projectId, "=").concat(token.value)
  } : {};
  return function fetchAllDocumentPath(selections) {
    var combinedSelections = (0, _optimizeQuery.combineSelections)(selections);
    return client.observable.fetch((0, _optimizeQuery.toQuery)(combinedSelections), {}, {
      tag: 'preview.document-paths',
      headers
    }).pipe((0, _operators.map)(result => (0, _optimizeQuery.reassemble)(result, combinedSelections)));
  };
}

var fetchDocumentPathsFast = (0, _debounceCollect.debounceCollect)(fetchAllDocumentPathsWith(_versionedClient.versionedClient), 100);
var fetchDocumentPathsSlow = (0, _debounceCollect.debounceCollect)(fetchAllDocumentPathsWith(_versionedClient.versionedClient), 1000);

function currentDatasetListenFields(id, fields) {
  return listen(id).pipe((0, _operators.switchMap)(event => {
    if (event.type === 'welcome' || event.visibility === 'query') {
      return fetchDocumentPathsFast(id, fields).pipe((0, _operators.mergeMap)(result => {
        return (0, _rxjs.concat)((0, _rxjs.of)(result), result === undefined // hack: if we get undefined as result here it can be because the document has
        ? // just been created and is not yet indexed. We therefore need to wait a bit
        // and then re-fetch.
        fetchDocumentPathsSlow(id, fields) : []);
      }));
    }

    return fetchDocumentPathsSlow(id, fields);
  }));
} // keep for debugging purposes for now
// function fetchDocumentPaths(id, selection) {
//   return client.observable.fetch(`*[_id==$id]{_id,_type,${selection.join(',')}}`, {id})
//     .map(result => result[0])
// }


var CACHE = {}; // todo: use a LRU cache instead (e.g. hashlru or quick-lru)

var getBatchFetcherForDataset = (0, _memoize2.default)(function getBatchFetcherForDataset(apiConfig, token) {
  var client = _versionedClient.versionedClient.withConfig(apiConfig);

  var fetchAll = fetchAllDocumentPathsWith(client, token);
  return (0, _debounceCollect.debounceCollect)(fetchAll, 10);
}, apiConfig => apiConfig.dataset + apiConfig.projectId);
var CROSS_DATASET_PREVIEW_POLL_INTERVAL = 10000; // We want to poll for changes in the other dataset, but only when window/tab is visible
// This sets up a shared stream that emits an event every `POLL_INTERVAL` milliseconds as long as the
// document is visible. It starts emitting immediately (if the page is visible)

var visiblePoll$ = (0, _rxjs.fromEvent)(document, 'visibilitychange').pipe((0, _operators.startWith)(0), (0, _operators.map)(() => document.visibilityState === 'visible'), (0, _operators.switchMap)(visible => visible ? (0, _rxjs.timer)(0, CROSS_DATASET_PREVIEW_POLL_INTERVAL) : _rxjs.EMPTY), (0, _operators.share)());

function crossDatasetListenFields(id, fields, apiConfig) {
  var token$ = (0, _.observePaths)((0, _crossProjectToken.getTokenDocumentId)({
    projectId: apiConfig.projectId
  }), ['token']).pipe((0, _operators.map)(document => document === null || document === void 0 ? void 0 : document.token));
  return (0, _rxjs.combineLatest)([token$, visiblePoll$.pipe((0, _operators.startWith)(0))]).pipe((0, _operators.switchMap)(_ref => {
    var _ref2 = _slicedToArray(_ref, 1),
        token = _ref2[0];

    var batchFetcher = getBatchFetcherForDataset(apiConfig, token ? {
      projectId: apiConfig.projectId,
      value: token
    } : undefined);
    return batchFetcher(id, fields);
  }));
}

function createCachedFieldObserver(id, fields, apiConfig) {
  var latest = null;
  var changes$ = (0, _rxjs.merge)((0, _rxjs.defer)(() => latest === null ? _rxjs.EMPTY : (0, _rxjs.of)(latest)), apiConfig ? crossDatasetListenFields(id, fields, apiConfig) : currentDatasetListenFields(id, fields)).pipe((0, _operators.tap)(v => latest = v), (0, _operators.publishReplay)(1), (0, _operators.refCount)());
  return {
    id,
    fields,
    changes$
  };
}

function cachedObserveFields(id, fields, apiConfig) {
  var cacheKey = apiConfig ? "".concat(apiConfig.projectId, ":").concat(apiConfig.dataset, ":").concat(id) : "$current$-".concat(id);

  if (!(cacheKey in CACHE)) {
    CACHE[cacheKey] = [];
  }

  var existingObservers = CACHE[cacheKey];
  var missingFields = (0, _difference2.default)(fields, (0, _flatten2.default)(existingObservers.map(cachedFieldObserver => cachedFieldObserver.fields)));

  if (missingFields.length > 0) {
    existingObservers.push(createCachedFieldObserver(id, fields, apiConfig));
  }

  var cachedFieldObservers = existingObservers.filter(observer => observer.fields.some(fieldName => fields.includes(fieldName))).map(cached => cached.changes$);
  return (0, _rxjs.combineLatest)(cachedFieldObservers).pipe( // in the event that a document gets deleted, the cached values will be updated to store `undefined`
  // if this happens, we should not pick any fields from it, but rather just return null
  (0, _operators.map)(snapshots => snapshots.filter(Boolean)), // make sure all snapshots agree on same revision
  (0, _operators.filter)(snapshots => (0, _isUniqueBy.default)(snapshots, snapshot => snapshot._rev)), // pass on value with the requested fields (or null if value is deleted)
  (0, _operators.map)(snapshots => snapshots.length === 0 ? null : pickFrom(snapshots, fields)), // emit values only if changed
  (0, _operators.distinctUntilChanged)((0, _hasEqualFields.default)(fields)));
}

function pickFrom(objects, fields) {
  return [..._constants.INCLUDE_FIELDS, ...fields].reduce((result, fieldName) => {
    var value = getFirstFieldValue(objects, fieldName);

    if (value !== undefined) {
      result[fieldName] = value;
    }

    return result;
  }, {});
}

function getFirstFieldValue(objects, fieldName) {
  var value;
  objects.some(object => {
    if (fieldName in object) {
      value = object[fieldName];
      return true;
    }

    return false;
  });
  return value;
}